{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnistCnn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smcd253/imgRecognition_cnn/blob/master/mnistCnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "37QK9gFLR_Om",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "0cd46f10-68a2-4b5e-dd30-d8e251a4fe3c"
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Training a Classifier\n",
        "=====================\n",
        "This is it. You have seen how to define neural networks, compute loss and make\n",
        "updates to the weights of the network.\n",
        "Now you might be thinking,\n",
        "What about data?\n",
        "----------------\n",
        "Generally, when you have to deal with image, text, audio or video data,\n",
        "you can use standard python packages that load data into a numpy array.\n",
        "Then you can convert this array into a ``torch.*Tensor``.\n",
        "-  For images, packages such as Pillow, OpenCV are useful\n",
        "-  For audio, packages such as scipy and librosa\n",
        "-  For text, either raw Python or Cython based loading, or NLTK and\n",
        "   SpaCy are useful\n",
        "Specifically for vision, we have created a package called\n",
        "``torchvision``, that has data loaders for common datasets such as\n",
        "Imagenet, CIFAR10, MNIST, etc. and data transformers for images, viz.,\n",
        "``torchvision.datasets`` and ``torch.utils.data.DataLoader``.\n",
        "This provides a huge convenience and avoids writing boilerplate code.\n",
        "For this tutorial, we will use the CIFAR10 dataset.\n",
        "It has the classes: ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’,\n",
        "‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’. The images in CIFAR-10 are of\n",
        "size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size.\n",
        ".. figure:: /_static/img/cifar10.png\n",
        "   :alt: cifar10\n",
        "   cifar10\n",
        "Training an image classifier\n",
        "----------------------------\n",
        "We will do the following steps in order:\n",
        "1. Load and normalizing the CIFAR10 training and test datasets using\n",
        "   ``torchvision``\n",
        "2. Define a Convolutional Neural Network\n",
        "3. Define a loss function\n",
        "4. Train the network on the training data\n",
        "5. Test the network on the test data\n",
        "1. Loading and normalizing CIFAR10\n",
        "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "Using ``torchvision``, it’s extremely easy to load CIFAR10.\n",
        "\"\"\"\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "########################################################################\n",
        "# The output of torchvision datasets are PILImage images of range [0, 1].\n",
        "# We transform them to Tensors of normalized range [-1, 1].\n",
        "\n",
        "batchSize = 1\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5,), (1.0,))])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='/Users/SwaggySpencerMcDee/Documents/ee569/MNIST', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batchSize,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='/Users/SwaggySpencerMcDee/Documents/ee569/MNIST', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batchSize,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = (0, 1, 2, 3, 4, 5, 6, 7, 8, 9)\n",
        "\n",
        "########################################################################\n",
        "# Let us show some of the training images, for fun.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# functions to show an image\n",
        "\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "print(' '.join('%5s' % classes[labels[j]] for j in range(batchSize)))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "    3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "urjbRG9jk5yo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "########################################################################\n",
        "# 2. Define a Convolutional Neural Network\n",
        "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "# Copy the neural network from the Neural Networks section before and modify it to\n",
        "# take 3-channel images (instead of 1-channel images as it was defined).\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5, stride = 1, padding = 0)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5, stride = 1, padding = 0)\n",
        "        self.fc1 = nn.Linear(256 * batchSize, 120)\n",
        "        self.fc2 = nn.Linear(120, 80)\n",
        "        self.fc3 = nn.Linear(80, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 256 * batchSize)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tkXujiPQnG9T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "########################################################################\n",
        "# 3. Define a Loss function and optimizer\n",
        "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "# Let's use a Classification Cross-Entropy loss and SGD with momentum.\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ztoCcZ9YnM-f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1054
        },
        "outputId": "ac135172-6bdc-4328-f1a5-a9066bf0a13b"
      },
      "cell_type": "code",
      "source": [
        "########################################################################\n",
        "# 4. Train the network\n",
        "# ^^^^^^^^^^^^^^^^^^^^\n",
        "#\n",
        "# This is when things start to get interesting.\n",
        "# We simply have to loop over our data iterator, and feed the inputs to the\n",
        "# network and optimize.\n",
        "\n",
        "#target GPU\n",
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(device)\n",
        "# net.to(device) # transfer netwrok to gpu\n",
        "\n",
        "# ADDED BY TA\n",
        "# net.train() # sets model to training mode (if you have drouout and BN, Batch Nom)\n",
        "\n",
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    # iterate over data\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        # transfer to gpu\n",
        "        # inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward() # back-propogate loss\n",
        "        optimizer.step() # update weight\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  2000] loss: 1.977\n",
            "[1,  4000] loss: 0.572\n",
            "[1,  6000] loss: 0.339\n",
            "[1,  8000] loss: 0.220\n",
            "[1, 10000] loss: 0.213\n",
            "[1, 12000] loss: 0.181\n",
            "[1, 14000] loss: 0.169\n",
            "[1, 16000] loss: 0.127\n",
            "[1, 18000] loss: 0.158\n",
            "[1, 20000] loss: 0.162\n",
            "[1, 22000] loss: 0.135\n",
            "[1, 24000] loss: 0.131\n",
            "[1, 26000] loss: 0.108\n",
            "[1, 28000] loss: 0.077\n",
            "[1, 30000] loss: 0.109\n",
            "[1, 32000] loss: 0.099\n",
            "[1, 34000] loss: 0.093\n",
            "[1, 36000] loss: 0.092\n",
            "[1, 38000] loss: 0.096\n",
            "[1, 40000] loss: 0.100\n",
            "[1, 42000] loss: 0.089\n",
            "[1, 44000] loss: 0.118\n",
            "[1, 46000] loss: 0.101\n",
            "[1, 48000] loss: 0.079\n",
            "[1, 50000] loss: 0.082\n",
            "[1, 52000] loss: 0.088\n",
            "[1, 54000] loss: 0.091\n",
            "[1, 56000] loss: 0.094\n",
            "[1, 58000] loss: 0.055\n",
            "[1, 60000] loss: 0.068\n",
            "[2,  2000] loss: 0.057\n",
            "[2,  4000] loss: 0.064\n",
            "[2,  6000] loss: 0.072\n",
            "[2,  8000] loss: 0.056\n",
            "[2, 10000] loss: 0.083\n",
            "[2, 12000] loss: 0.057\n",
            "[2, 14000] loss: 0.063\n",
            "[2, 16000] loss: 0.079\n",
            "[2, 18000] loss: 0.070\n",
            "[2, 20000] loss: 0.061\n",
            "[2, 22000] loss: 0.070\n",
            "[2, 24000] loss: 0.066\n",
            "[2, 26000] loss: 0.068\n",
            "[2, 28000] loss: 0.076\n",
            "[2, 30000] loss: 0.050\n",
            "[2, 32000] loss: 0.082\n",
            "[2, 34000] loss: 0.063\n",
            "[2, 36000] loss: 0.058\n",
            "[2, 38000] loss: 0.062\n",
            "[2, 40000] loss: 0.070\n",
            "[2, 42000] loss: 0.064\n",
            "[2, 44000] loss: 0.060\n",
            "[2, 46000] loss: 0.055\n",
            "[2, 48000] loss: 0.068\n",
            "[2, 50000] loss: 0.054\n",
            "[2, 52000] loss: 0.054\n",
            "[2, 54000] loss: 0.053\n",
            "[2, 56000] loss: 0.072\n",
            "[2, 58000] loss: 0.070\n",
            "[2, 60000] loss: 0.054\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kQigUwf4tef4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "f8545996-3f46-4e5f-91be-26d4247776fd"
      },
      "cell_type": "code",
      "source": [
        "#######################################################################\n",
        "# 5. Test the network on the test data\n",
        "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "#\n",
        "# We have trained the network for 2 passes over the training dataset.\n",
        "# But we need to check if the network has learnt anything at all.\n",
        "#\n",
        "# We will check this by predicting the class label that the neural network\n",
        "# outputs, and checking it against the ground-truth. If the prediction is\n",
        "# correct, we add the sample to the list of correct predictions.\n",
        "#\n",
        "# Okay, first step. Let us display an image from the test set to get familiar.\n",
        "\n",
        "dataiter = iter(testloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# print images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(batchSize)))\n",
        "\n",
        "########################################################################\n",
        "# Okay, now let us see what the neural network thinks these examples above are:\n",
        "\n",
        "outputs = net(images)\n",
        "\n",
        "########################################################################\n",
        "# The outputs are energies for the 10 classes.\n",
        "# The higher the energy for a class, the more the network\n",
        "# thinks that the image is of the particular class.\n",
        "# So, let's get the index of the highest energy:\n",
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
        "                              for j in range(batchSize)))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEJYAYPBfEeNj1obLszsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "GroundTruth:      7\n",
            "Predicted:      7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ckXcEEm5tmHT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "76b045e1-aecb-4cba-f6a7-c1444813446b"
      },
      "cell_type": "code",
      "source": [
        "########################################################################\n",
        "# The results seem pretty good.\n",
        "#\n",
        "# Let us look at how the network performs on the whole dataset.\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "# net.eval()\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 98 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}